{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3c23e14",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f0d63a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33179c23",
   "metadata": {},
   "source": [
    "### Sample Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6072dde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc = \"Life is like a camera. Focus on what's important, capture the good times, develop from the negatives, and if things don't work out, take another shot.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4289c865",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be5501c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Doc/text :\n",
      " Life is like a camera. Focus on what's important, capture the good times, develop from the negatives, and if things don't work out, take another shot.\n",
      "\n",
      "\n",
      "Tokenized Text :\n",
      " ['Life', 'is', 'like', 'a', 'camera', '.', 'Focus', 'on', 'what', \"'s\", 'important', ',', 'capture', 'the', 'good', 'times', ',', 'develop', 'from', 'the', 'negatives', ',', 'and', 'if', 'things', 'do', \"n't\", 'work', 'out', ',', 'take', 'another', 'shot', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_words = word_tokenize(sample_doc)\n",
    "print(\"Original Doc/text :\\n\",sample_doc)\n",
    "print(\"\\n\\nTokenized Text :\\n\",tokenized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4892a4",
   "metadata": {},
   "source": [
    "### Stop Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "417b4146",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "#Printing the stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7eb234f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentence without stopword removal :\n",
      " ['Life', 'is', 'like', 'a', 'camera', '.', 'Focus', 'on', 'what', \"'s\", 'important', ',', 'capture', 'the', 'good', 'times', ',', 'develop', 'from', 'the', 'negatives', ',', 'and', 'if', 'things', 'do', \"n't\", 'work', 'out', ',', 'take', 'another', 'shot', '.']\n",
      "\n",
      "\n",
      "Tokenized sentence with stopword removal :\n",
      " ['Life', 'like', 'camera', '.', 'Focus', \"'s\", 'important', ',', 'capture', 'good', 'times', ',', 'develop', 'negatives', ',', 'things', \"n't\", 'work', ',', 'take', 'another', 'shot', '.']\n"
     ]
    }
   ],
   "source": [
    "cleaned_token = []\n",
    "for word in tokenized_words:\n",
    "    if word not in stop_words:\n",
    "        cleaned_token.append(word)\n",
    "        \n",
    "print(\"Tokenized sentence without stopword removal :\\n\",tokenized_words)\n",
    "print(\"\\n\\nTokenized sentence with stopword removal :\\n\",cleaned_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80449cec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['life', 'like', 'camera', 'focus', 'important', 'capture', 'good', 'times', 'develop', 'negatives', 'things', 'work', 'take', 'another', 'shot']\n"
     ]
    }
   ],
   "source": [
    "#Cleaning\n",
    "words = [cleaned_token.lower() for cleaned_token in cleaned_token if cleaned_token.isalpha()] \n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ada4a48",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15b050ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['life', 'like', 'camera', 'focu', 'import', 'captur', 'good', 'time', 'develop', 'neg', 'thing', 'work', 'take', 'anoth', 'shot']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "port_stemmer_output = [stemmer.stem(words) for words in words]\n",
    "print(port_stemmer_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d645e1",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02f5b6e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['life', 'like', 'camera', 'focus', 'important', 'capture', 'good', 'time', 'develop', 'negative', 'thing', 'work', 'take', 'another', 'shot']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer() \n",
    "lemmatizer_output = [lemmatizer.lemmatize(words) for words in words] \n",
    "print(lemmatizer_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8efaec3",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbff354b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Life', 'NNP'), ('like', 'IN'), ('camera', 'NN'), ('.', '.'), ('Focus', 'NNP'), (\"'s\", 'POS'), ('important', 'JJ'), (',', ','), ('capture', 'NN'), ('good', 'JJ'), ('times', 'NNS'), (',', ','), ('develop', 'VB'), ('negatives', 'NNS'), (',', ','), ('things', 'NNS'), (\"n't\", 'RB'), ('work', 'VB'), (',', ','), ('take', 'VB'), ('another', 'DT'), ('shot', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged = pos_tag(cleaned_token)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59da314",
   "metadata": {},
   "source": [
    "### Calculation of Term Frequency and Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82d566fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = [\"The quick brown fox jumps over the lazy dog\",\n",
    "       \"The lazy cat sleeps on the brown rug\",\n",
    "       \"Brown bears are common in this area\",\n",
    "       \"The quick fox runs faster than the brown dog\",\n",
    "       \"The lazy dog lies down under the brown tree\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc5e3fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the: 21\n",
      "quick: 16\n",
      "brown: 3\n",
      "fox: 9\n",
      "jumps: 11\n",
      "over: 15\n",
      "lazy: 12\n",
      "dog: 6\n",
      "cat: 4\n",
      "sleeps: 19\n",
      "on: 14\n",
      "rug: 17\n",
      "bears: 2\n",
      "are: 0\n",
      "common: 5\n",
      "in: 10\n",
      "this: 22\n",
      "area: 1\n",
      "runs: 18\n",
      "faster: 8\n",
      "than: 20\n",
      "lies: 13\n",
      "down: 7\n",
      "under: 24\n",
      "tree: 23\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer = \"word\", norm = None, use_idf = True, smooth_idf = True)\n",
    "Mat = vectorizer.fit(doc)\n",
    "vocabulary = Mat.vocabulary_\n",
    "\n",
    "for word, number in vocabulary.items():\n",
    "    print(f\"{word}: {number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebf05029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6)\t1.4054651081081644\n",
      "  (0, 12)\t1.4054651081081644\n",
      "  (0, 15)\t2.09861228866811\n",
      "  (0, 11)\t2.09861228866811\n",
      "  (0, 9)\t1.6931471805599454\n",
      "  (0, 3)\t1.0\n",
      "  (0, 16)\t1.6931471805599454\n",
      "  (0, 21)\t2.3646431135879094\n",
      "  (1, 17)\t2.09861228866811\n",
      "  (1, 14)\t2.09861228866811\n",
      "  (1, 19)\t2.09861228866811\n",
      "  (1, 4)\t2.09861228866811\n",
      "  (1, 12)\t1.4054651081081644\n",
      "  (1, 3)\t1.0\n",
      "  (1, 21)\t2.3646431135879094\n",
      "  (2, 1)\t2.09861228866811\n",
      "  (2, 22)\t2.09861228866811\n",
      "  (2, 10)\t2.09861228866811\n",
      "  (2, 5)\t2.09861228866811\n",
      "  (2, 0)\t2.09861228866811\n",
      "  (2, 2)\t2.09861228866811\n",
      "  (2, 3)\t1.0\n",
      "  (3, 20)\t2.09861228866811\n",
      "  (3, 8)\t2.09861228866811\n",
      "  (3, 18)\t2.09861228866811\n",
      "  (3, 6)\t1.4054651081081644\n",
      "  (3, 9)\t1.6931471805599454\n",
      "  (3, 3)\t1.0\n",
      "  (3, 16)\t1.6931471805599454\n",
      "  (3, 21)\t2.3646431135879094\n",
      "  (4, 23)\t2.09861228866811\n",
      "  (4, 24)\t2.09861228866811\n",
      "  (4, 7)\t2.09861228866811\n",
      "  (4, 13)\t2.09861228866811\n",
      "  (4, 6)\t1.4054651081081644\n",
      "  (4, 12)\t1.4054651081081644\n",
      "  (4, 3)\t1.0\n",
      "  (4, 21)\t2.3646431135879094\n"
     ]
    }
   ],
   "source": [
    "tfidfMat = vectorizer.fit_transform(doc)\n",
    "print(tfidfMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e3ec07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['are' 'area' 'bears' 'brown' 'cat' 'common' 'dog' 'down' 'faster' 'fox'\n",
      " 'in' 'jumps' 'lazy' 'lies' 'on' 'over' 'quick' 'rug' 'runs' 'sleeps'\n",
      " 'than' 'the' 'this' 'tree' 'under']\n"
     ]
    }
   ],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62370fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = tfidfMat.todense()\n",
    "denselist = dense.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f770f9bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>are</th>\n",
       "      <th>area</th>\n",
       "      <th>bears</th>\n",
       "      <th>brown</th>\n",
       "      <th>cat</th>\n",
       "      <th>common</th>\n",
       "      <th>dog</th>\n",
       "      <th>down</th>\n",
       "      <th>faster</th>\n",
       "      <th>fox</th>\n",
       "      <th>...</th>\n",
       "      <th>over</th>\n",
       "      <th>quick</th>\n",
       "      <th>rug</th>\n",
       "      <th>runs</th>\n",
       "      <th>sleeps</th>\n",
       "      <th>than</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "      <th>tree</th>\n",
       "      <th>under</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>...</td>\n",
       "      <td>2.098612</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.364643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.364643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.098612</td>\n",
       "      <td>2.098612</td>\n",
       "      <td>2.098612</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.098612</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.098612</td>\n",
       "      <td>2.364643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.405465</td>\n",
       "      <td>2.098612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.364643</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.098612</td>\n",
       "      <td>2.098612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        are      area     bears  brown       cat    common       dog  \\\n",
       "0  0.000000  0.000000  0.000000    1.0  0.000000  0.000000  1.405465   \n",
       "1  0.000000  0.000000  0.000000    1.0  2.098612  0.000000  0.000000   \n",
       "2  2.098612  2.098612  2.098612    1.0  0.000000  2.098612  0.000000   \n",
       "3  0.000000  0.000000  0.000000    1.0  0.000000  0.000000  1.405465   \n",
       "4  0.000000  0.000000  0.000000    1.0  0.000000  0.000000  1.405465   \n",
       "\n",
       "       down    faster       fox  ...      over     quick       rug      runs  \\\n",
       "0  0.000000  0.000000  1.693147  ...  2.098612  1.693147  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  ...  0.000000  0.000000  2.098612  0.000000   \n",
       "2  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  2.098612  1.693147  ...  0.000000  1.693147  0.000000  2.098612   \n",
       "4  2.098612  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "     sleeps      than       the      this      tree     under  \n",
       "0  0.000000  0.000000  2.364643  0.000000  0.000000  0.000000  \n",
       "1  2.098612  0.000000  2.364643  0.000000  0.000000  0.000000  \n",
       "2  0.000000  0.000000  0.000000  2.098612  0.000000  0.000000  \n",
       "3  0.000000  2.098612  2.364643  0.000000  0.000000  0.000000  \n",
       "4  0.000000  0.000000  2.364643  0.000000  2.098612  2.098612  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating Pandas Dataframe of the feature names and there TFIDF values\n",
    "df = pd.DataFrame(denselist,columns = feature_names)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
